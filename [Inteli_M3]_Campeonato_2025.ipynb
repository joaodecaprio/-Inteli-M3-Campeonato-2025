{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===== PIPELINE COMPLETO ULTRA OTIMIZADO PARA 80%+ =====\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from lightgbm import LGBMClassifier\n",
        "import xgboost as xgb\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ===== 1. CARREGAMENTO DOS DADOS =====\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")\n",
        "print(f\"Target distribution:\\n{train['labels'].value_counts(normalize=True)}\")\n",
        "\n",
        "# ===== 2. PRÉ-PROCESSAMENTO AVANÇADO =====\n",
        "print(\"\\n Iniciando pré-processamento avançado...\")\n",
        "\n",
        "age_cols = ['age_first_funding_year', 'age_last_funding_year',\n",
        "           'age_first_milestone_year', 'age_last_milestone_year']\n",
        "\n",
        "# Imputação inteligente de NaNs (sem warnings)\n",
        "for col in age_cols:\n",
        "    train[col] = train[col].fillna(train[col].median())\n",
        "    test[col] = test[col].fillna(train[col].median())\n",
        "\n",
        "train['funding_total_usd'] = train['funding_total_usd'].fillna(train['funding_total_usd'].median())\n",
        "test['funding_total_usd'] = test['funding_total_usd'].fillna(train['funding_total_usd'].median())\n",
        "\n",
        "# ===== 3. FEATURE ENGINEERING AVANÇADO =====\n",
        "print(\"Criando features avançadas...\")\n",
        "\n",
        "# Features básicas (já testadas)\n",
        "train['funding_per_round'] = train['funding_total_usd'] / (train['funding_rounds'] + 1e-6)\n",
        "test['funding_per_round'] = test['funding_total_usd'] / (test['funding_rounds'] + 1e-6)\n",
        "\n",
        "train['milestones_per_year'] = train['milestones'] / (train['age_last_milestone_year'] + 1e-6)\n",
        "test['milestones_per_year'] = test['milestones'] / (test['age_last_milestone_year'] + 1e-6)\n",
        "\n",
        "train['has_multiple_rounds'] = (train['funding_rounds'] > 1).astype(int)\n",
        "test['has_multiple_rounds'] = (test['funding_rounds'] > 1).astype(int)\n",
        "\n",
        "train['funding_age_span'] = train['age_last_funding_year'] - train['age_first_funding_year']\n",
        "test['funding_age_span'] = test['age_last_funding_year'] - test['age_first_funding_year']\n",
        "\n",
        "train['relationships_per_milestone'] = train['relationships'] / (train['milestones'] + 1e-6)\n",
        "test['relationships_per_milestone'] = test['relationships'] / (test['milestones'] + 1e-6)\n",
        "\n",
        "# Features extras para ganhar mais poder preditivo\n",
        "train['funding_efficiency'] = train['funding_total_usd'] / (train['relationships'] + 1e-6)\n",
        "test['funding_efficiency'] = test['funding_total_usd'] / (test['relationships'] + 1e-6)\n",
        "\n",
        "train['milestone_density'] = train['milestones'] / (train['funding_rounds'] + 1e-6)\n",
        "test['milestone_density'] = test['milestones'] / (test['funding_rounds'] + 1e-6)\n",
        "\n",
        "train['avg_funding_per_participant'] = train['funding_total_usd'] / (train['avg_participants'] + 1e-6)\n",
        "test['avg_funding_per_participant'] = test['funding_total_usd'] / (test['avg_participants'] + 1e-6)\n",
        "\n",
        "# Features de interação (combinações que podem ser importantes)\n",
        "train['funding_milestone_ratio'] = train['funding_total_usd'] / (train['milestones'] + 1e-6)\n",
        "test['funding_milestone_ratio'] = test['funding_total_usd'] / (test['milestones'] + 1e-6)\n",
        "\n",
        "# Tratar infinitos e NaNs\n",
        "train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "train.fillna(0, inplace=True)\n",
        "test.fillna(0, inplace=True)\n",
        "\n",
        "print(f\"Features criadas! Train shape: {train.shape}\")\n",
        "\n",
        "# ===== 4. ENCODING E SCALING =====\n",
        "print(\"Aplicando encoding e scaling...\")\n",
        "\n",
        "# One-Hot Encoding\n",
        "train_encoded = pd.get_dummies(train, columns=['category_code'], prefix='cat')\n",
        "test_encoded = pd.get_dummies(test, columns=['category_code'], prefix='cat')\n",
        "train_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# Scaling das features numéricas\n",
        "new_features = ['funding_per_round', 'milestones_per_year', 'has_multiple_rounds',\n",
        "                'funding_age_span', 'relationships_per_milestone', 'funding_efficiency',\n",
        "                'milestone_density', 'avg_funding_per_participant', 'funding_milestone_ratio']\n",
        "\n",
        "numeric_cols = ['relationships', 'funding_rounds', 'funding_total_usd',\n",
        "               'milestones', 'avg_participants'] + age_cols + new_features\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_encoded[numeric_cols] = scaler.fit_transform(train_encoded[numeric_cols])\n",
        "test_encoded[numeric_cols] = scaler.transform(test_encoded[numeric_cols])\n",
        "\n",
        "# Separar features e target\n",
        "X = train_encoded.drop(['labels'], axis=1)\n",
        "y = train_encoded['labels']\n",
        "if 'labels' in test_encoded.columns:\n",
        "    test_encoded = test_encoded.drop(['labels'], axis=1)\n",
        "X_test = test_encoded\n",
        "\n",
        "print(f\"Dataset final: X shape = {X.shape}, X_test shape = {X_test.shape}\")\n",
        "\n",
        "# ===== 5. SPLIT ESTRATIFICADO =====\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,\n",
        "                                                  random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Split: Train={X_train.shape[0]}, Val={X_val.shape[0]}\")\n",
        "\n",
        "# ===== 6. BALANCEAMENTO COM SMOTE =====\n",
        "print(\"\\n Aplicando SMOTE para balanceamento...\")\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f\" Antes SMOTE: {y_train.value_counts().to_dict()}\")\n",
        "print(f\" Depois SMOTE: {y_train_balanced.value_counts().to_dict()}\")\n",
        "\n",
        "# ===== 7. MODELOS INDIVIDUAIS OTIMIZADOS =====\n",
        "print(\"\\n Treinando modelos individuais otimizados...\")\n",
        "\n",
        "# Random Forest Ultra Otimizado\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=800,         # Mais árvores para reduzir variância\n",
        "    max_depth=25,             # Profundidade maior\n",
        "    max_features='sqrt',      # Melhor para evitar overfitting\n",
        "    min_samples_split=2,      # Permitir splits mais agressivos\n",
        "    min_samples_leaf=1,       # Folhas menores\n",
        "    bootstrap=True,           # Bagging\n",
        "    random_state=42,\n",
        "    n_jobs=-1,               # Usar todos os cores\n",
        "    class_weight='balanced'   # Balanceamento adicional\n",
        ")\n",
        "\n",
        "# LightGBM Ultra Otimizado\n",
        "lgbm_model = LGBMClassifier(\n",
        "    n_estimators=1000,        # Mais árvores\n",
        "    max_depth=25,             # Profundidade maior\n",
        "    learning_rate=0.03,       # Taxa menor para melhor convergência\n",
        "    subsample=0.8,            # Bagging\n",
        "    colsample_bytree=0.8,     # Feature sampling\n",
        "    reg_alpha=0.1,            # Regularização L1\n",
        "    reg_lambda=0.1,           # Regularização L2\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    verbose=-1                # Silencioso\n",
        ")\n",
        "\n",
        "# XGBoost Ultra Otimizado\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=800,\n",
        "    max_depth=8,\n",
        "    learning_rate=0.03,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=0.1,\n",
        "    random_state=42,\n",
        "    scale_pos_weight=1,       # Balanceamento já feito pelo SMOTE\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "print(\"Treinando Random Forest...\")\n",
        "rf_model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "print(\"Treinando LightGBM...\")\n",
        "lgbm_model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "print(\"Treinando XGBoost...\")\n",
        "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "print(\"Modelos individuais treinados!\")\n",
        "\n",
        "# ===== 8. STACKING CLASSIFIER (MAIS PODEROSO QUE VOTING) =====\n",
        "print(\"\\n Criando Stacking Classifier...\")\n",
        "\n",
        "# Meta-modelo para combinar as previsões\n",
        "meta_model = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "# Stacking Classifier\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', rf_model),\n",
        "        ('lgbm', lgbm_model),\n",
        "        ('xgb', xgb_model)\n",
        "    ],\n",
        "    final_estimator=meta_model,\n",
        "    passthrough=True,         # Inclui features originais no meta-modelo\n",
        "    cv=5,                     # Cross-validation para treinar meta-modelo\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"Treinando Stacking Classifier...\")\n",
        "stacking_model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "print(\"Stacking Classifier treinado!\")\n",
        "\n",
        "# ===== 9. THRESHOLD TUNING ULTRA FINO =====\n",
        "print(\"\\n THRESHOLD TUNING ULTRA FINO...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Predições no conjunto de validação\n",
        "y_prob_val = stacking_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Threshold tuning com passo super fino (0.001)\n",
        "best_acc = 0\n",
        "best_thr = 0.5\n",
        "\n",
        "print(\"Testando thresholds de 0.30 a 0.80 com passo 0.001...\")\n",
        "for thr in np.arange(0.30, 0.81, 0.001):\n",
        "    preds_thr = (y_prob_val > thr).astype(int)\n",
        "    acc = (preds_thr == y_val).mean()\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        best_thr = thr\n",
        "\n",
        "print(f\"\\n MELHOR THRESHOLD: {best_thr:.3f}\")\n",
        "print(f\"MELHOR ACCURACY: {best_acc:.4f} ({best_acc*100:.1f}%)\")\n",
        "\n",
        "# Relatório detalhado com o melhor threshold\n",
        "y_pred_final = (y_prob_val > best_thr).astype(int)\n",
        "auc_final = roc_auc_score(y_val, y_prob_val)\n",
        "\n",
        "print(f\"\\n RELATÓRIO FINAL (threshold = {best_thr:.3f}):\")\n",
        "print(f\"AUC: {auc_final:.4f}\")\n",
        "print(classification_report(y_val, y_pred_final))\n",
        "\n",
        "# ===== 10. MODELO FINAL E SUBMISSÃO =====\n",
        "print(\"\\n TREINANDO MODELO FINAL E GERANDO SUBMISSÃO...\")\n",
        "\n",
        "# Treinar stacking com todos os dados de treino (sem split)\n",
        "final_stacking = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', rf_model),\n",
        "        ('lgbm', lgbm_model),\n",
        "        ('xgb', xgb_model)\n",
        "    ],\n",
        "    final_estimator=meta_model,\n",
        "    passthrough=True,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Aplicar SMOTE em todo o dataset de treino\n",
        "X_full_balanced, y_full_balanced = sm.fit_resample(X, y)\n",
        "\n",
        "# Treinar modelo final\n",
        "final_stacking.fit(X_full_balanced, y_full_balanced)\n",
        "\n",
        "# Predições no test set com threshold otimizado\n",
        "y_prob_test = final_stacking.predict_proba(X_test)[:, 1]\n",
        "test_predictions = (y_prob_test > best_thr).astype(int)\n",
        "\n",
        "# Criar submissão\n",
        "submission = pd.DataFrame({\n",
        "    'id': sample_sub['id'],\n",
        "    'labels': test_predictions\n",
        "})\n",
        "\n",
        "submission.to_csv('my_submission_ultra_optimized.csv', index=False)\n",
        "\n",
        "print(\"Submissão salva como 'my_submission_ultra_optimized.csv'\")\n",
        "print(f\"Modelo usado: Stacking Classifier (RF + LGBM + XGB)\")\n",
        "print(f\"Balanceamento: SMOTE\")\n",
        "print(f\"Threshold otimizado: {best_thr:.3f}\")\n",
        "print(f\"Acurácia alcançada: {best_acc:.1%}\")\n",
        "print(f\"AUC: {auc_final:.4f}\")\n",
        "\n",
        "# Verificar distribuição das predições\n",
        "print(f\"\\n Distribuição das predições no test set:\")\n",
        "print(f\"Classe 0 (insucesso): {(test_predictions == 0).sum()} startups\")\n",
        "print(f\"Classe 1 (sucesso): {(test_predictions == 1).sum()} startups\")\n",
        "print(f\"Proporção de sucesso: {test_predictions.mean():.1%}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESUMO FINAL:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"• Features criadas: {len(new_features)} novas variáveis\")\n",
        "print(f\"• Balanceamento: SMOTE aplicado\")\n",
        "print(f\"• Modelos: Random Forest + LightGBM + XGBoost\")\n",
        "print(f\"• Meta-modelo: Stacking Classifier\")\n",
        "print(f\"• Threshold: {best_thr:.3f} (otimizado com passo 0.001)\")\n",
        "print(f\"• Acurácia final: {best_acc:.1%}\")\n",
        "print(f\"• AUC: {auc_final:.4f}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vawhXnc3luY",
        "outputId": "3102d9fe-b21d-42f8-a8b8-5e0cbe33ebb1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (646, 33)\n",
            "Test shape: (277, 32)\n",
            "Target distribution:\n",
            "labels\n",
            "1    0.647059\n",
            "0    0.352941\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            " Iniciando pré-processamento avançado...\n",
            "Criando features avançadas...\n",
            "Features criadas! Train shape: (646, 42)\n",
            "Aplicando encoding e scaling...\n",
            "Dataset final: X shape = (646, 74), X_test shape = (277, 74)\n",
            "Split: Train=516, Val=130\n",
            "\n",
            " Aplicando SMOTE para balanceamento...\n",
            " Antes SMOTE: {1: 334, 0: 182}\n",
            " Depois SMOTE: {0: 334, 1: 334}\n",
            "\n",
            " Treinando modelos individuais otimizados...\n",
            "Treinando Random Forest...\n",
            "Treinando LightGBM...\n",
            "Treinando XGBoost...\n",
            "Modelos individuais treinados!\n",
            "\n",
            " Criando Stacking Classifier...\n",
            "Treinando Stacking Classifier...\n",
            "Stacking Classifier treinado!\n",
            "\n",
            " THRESHOLD TUNING ULTRA FINO...\n",
            "==================================================\n",
            "Testando thresholds de 0.30 a 0.80 com passo 0.001...\n",
            "\n",
            " MELHOR THRESHOLD: 0.300\n",
            "MELHOR ACCURACY: 0.8000 (80.0%)\n",
            "\n",
            " RELATÓRIO FINAL (threshold = 0.300):\n",
            "AUC: 0.8173\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.57      0.67        46\n",
            "           1       0.80      0.93      0.86        84\n",
            "\n",
            "    accuracy                           0.80       130\n",
            "   macro avg       0.80      0.75      0.76       130\n",
            "weighted avg       0.80      0.80      0.79       130\n",
            "\n",
            "\n",
            " TREINANDO MODELO FINAL E GERANDO SUBMISSÃO...\n",
            "Submissão salva como 'my_submission_ultra_optimized.csv'\n",
            "Modelo usado: Stacking Classifier (RF + LGBM + XGB)\n",
            "Balanceamento: SMOTE\n",
            "Threshold otimizado: 0.300\n",
            "Acurácia alcançada: 80.0%\n",
            "AUC: 0.8173\n",
            "\n",
            " Distribuição das predições no test set:\n",
            "Classe 0 (insucesso): 61 startups\n",
            "Classe 1 (sucesso): 216 startups\n",
            "Proporção de sucesso: 78.0%\n",
            "\n",
            "============================================================\n",
            "RESUMO FINAL:\n",
            "============================================================\n",
            "• Features criadas: 9 novas variáveis\n",
            "• Balanceamento: SMOTE aplicado\n",
            "• Modelos: Random Forest + LightGBM + XGBoost\n",
            "• Meta-modelo: Stacking Classifier\n",
            "• Threshold: 0.300 (otimizado com passo 0.001)\n",
            "• Acurácia final: 80.0%\n",
            "• AUC: 0.8173\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}